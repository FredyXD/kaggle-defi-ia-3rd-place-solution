{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Nous utilisions la librairie simpletransformers qui rend l'utilisation des modeles de type transformers super simple\n",
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json('data/train.json').set_index('Id')\n",
    "data = data.drop('gender', axis=1)\n",
    "target = pd.read_csv('data/train_label.csv')\n",
    "data['label'] = target['Category']\n",
    "data.columns = ['text', 'label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = ClassificationArgs()\n",
    "model_args.evaluate_during_training = False\n",
    "\n",
    "# La recherche des parametres optimaux a ete faite avec l'application weights and biases, voir la section Best hyperparameter search\n",
    "model_args.manual_seed = 2\n",
    "model_args.learning_rate = 1.850322830319812e-05\n",
    "model_args.num_train_epochs = 2\n",
    "model_args.max_seq_length = 256\n",
    "model_args.multiprocessing_chunksize = 5000\n",
    "model_args.no_cache = True\n",
    "model_args.no_save = True\n",
    "model_args.reprocess_input_data = True\n",
    "model_args.train_batch_size = 24\n",
    "model_args.gradient_accumulation_steps = 2\n",
    "model_args.train_custom_parameters_only = False\n",
    "\n",
    "# Le modele transformers utilise est Roberta Large, pour le faire tourner il faut une carte avec plus de 16GB de memoire\n",
    "model = ClassificationModel(\n",
    "        \"roberta\",\n",
    "        \"roberta-large\",\n",
    "        num_labels=28,\n",
    "        use_cuda=True,\n",
    "        args=model_args\n",
    "    )\n",
    "\n",
    "model.train_model(\n",
    "        data\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_json('data/test.json')\n",
    "predictions = model.predict(test.description.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"Category\"] = predictions[0]\n",
    "predictionsDf = test[[\"Id\",\"Category\"]]\n",
    "predictionsDf.to_csv(\"soumissions/simple_transformers_roberta_large_1.8_2epoch.csv\", index=False)\n",
    "predictionsDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login(key='enter key here')\n",
    "\n",
    "train_df, test_df = train_test_split(data, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = ClassificationArgs()\n",
    "model_args.eval_batch_size = 24\n",
    "model_args.evaluate_during_training = True\n",
    "model_args.evaluate_during_training_silent = False\n",
    "model_args.evaluate_during_training_steps = 2300\n",
    "model_args.manual_seed = 4\n",
    "model_args.max_seq_length = 256\n",
    "model_args.multiprocessing_chunksize = 5000\n",
    "model_args.no_cache = True\n",
    "model_args.no_save = True\n",
    "model_args.num_train_epochs = 2\n",
    "model_args.output_dir = '/save'\n",
    "model_args.overwrite_output_dir = True\n",
    "model_args.reprocess_input_data = True\n",
    "model_args.train_batch_size = 24\n",
    "model_args.gradient_accumulation_steps = 2\n",
    "model_args.train_custom_parameters_only = False\n",
    "\n",
    "sweep_config = {\n",
    "    \"name\": \"vanilla-sweep-batch-16\",\n",
    "    \"method\": \"bayes\",\n",
    "    \"metric\": {\"name\": \"f1_score\", \"goal\": \"maximize\"},\n",
    "    \"parameters\": {\n",
    "        \"num_train_epochs\": {\"min\": 1, \"max\": 3},\n",
    "        \"learning_rate\": {\"min\": 1e-05, \"max\": 2.5e-05},\n",
    "    },\n",
    "    \"early_terminate\": {\"type\": \"hyperband\", \"min_iter\": 6,},\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"DEFI IA - Hyperparameter Optimization\")\n",
    "\n",
    "def f1_multiclass(labels, preds):\n",
    "    return f1_score(labels, preds, average='macro')\n",
    "\n",
    "def train():\n",
    "    # Initialize a new wandb run\n",
    "    wandb.init()\n",
    "\n",
    "    # Create a TransformerModel\n",
    "    global model\n",
    "    model = ClassificationModel(\n",
    "        \"roberta\",\n",
    "        \"roberta-large\",\n",
    "        num_labels=28,\n",
    "        use_cuda=True,\n",
    "        args=model_args,\n",
    "        sweep_config=wandb.config,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    model.train_model(\n",
    "        train_df,\n",
    "        eval_df=test_df,\n",
    "        f1_score=f1_multiclass\n",
    "        )\n",
    "\n",
    "    # Sync wandb\n",
    "    wandb.join()\n",
    "\n",
    "wandb.agent(sweep_id, train)\n",
    "\n",
    "# Cette recherche n'a pas pu etre completement executee car tres lente, 1 seule epoch faisait environ 1h20 sur une rtx 6000\n",
    "# Les resultats peuvent etres visualises sur pdf, autre piece jointe du mail"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
